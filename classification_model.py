# -*- coding: utf-8 -*-
"""Classification model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j7cw23mF81peTN3K8EIcWmvOwWJjzpn2

Dataset balancing
"""

import pandas as pd
df_output_data_new = pd.read_csv('output_data_new.txt', delimiter='\t' )
df_output_data_new.gender.value_counts()

male_rows = df_output_data_new[df_output_data_new['gender'] == 'm']
female_rows = df_output_data_new[df_output_data_new['gender'] == 'f']

df_male = male_rows.iloc[0:13500]
df_male

combined_df = pd.concat([df_male , female_rows], ignore_index=True)
combined_df

"""Data pre-processing"""

import numpy as np
df = pd.read_csv('LFM-1b_users_additional.txt', delimiter='\t' )
df_add = df.rename (columns={'user-id':'user_id'})
data  = pd.merge(df_add, combined_df, on='user_id')
data = data.replace('?',np.NaN)
data = data.dropna()

"""1. Logistic Regression using 2 independent features"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

# Select features (listening events and count of artists) and target variable (gender)
X = data[['cnt_listeningevents', 'cnt_distinct_artists']]
y = data['gender']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Standardize features (to improve model performance)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train a logistic regression model
model = LogisticRegression(random_state=42)
model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)

# Calculate accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Evaluate model performance
print(classification_report(y_test, y_pred))

# Get feature importances (coefficients) from the trained model
feature_importances = model.coef_[0]

# Create a DataFrame to display feature importances
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

print("Feature Importances:")
print(importance_df)

"""2. Logistic Regression using all 5 independent features - Best Accuracy"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

# Select features (listening events and count of artists) and target variable (gender)
X = data[['cnt_listeningevents', 'cnt_distinct_artists','cnt_distinct_tracks','novelty_artist_avg_year','mainstreaminess_avg_year' ]]
y = data['gender']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Standardize features (to improve model performance)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train a logistic regression model
model = LogisticRegression(random_state=42)
model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)

# Calculate accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Evaluate model performance
print(classification_report(y_test, y_pred))

# Get feature importances (coefficients) from the trained model
feature_importances = model.coef_[0]

# Create a DataFrame to display feature importances
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

print("Feature Importances:")
print(importance_df)

"""3. SVM with all 5 features"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC  # Import the Support Vector Classifier
from sklearn.metrics import accuracy_score


# Select features (listening events and count of artists) and target variable (gender)
X = data[['cnt_listeningevents', 'cnt_distinct_artists', 'cnt_distinct_tracks', 'novelty_artist_avg_year', 'mainstreaminess_avg_year']]
y = data['gender']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Standardize features (to improve model performance)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train a Support Vector Machine (SVM) model
model = SVC(random_state=42)
model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)

# Calculate accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Evaluate model performance
print(classification_report(y_test, y_pred))

"""4. K  Means clustering with all 5 features"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans  # Import KMeans clustering
from sklearn.metrics import silhouette_score
from sklearn.metrics import classification_report

# Select features (listening events and count of artists) and target variable (gender)
X = data[['cnt_listeningevents', 'cnt_distinct_artists', 'cnt_distinct_tracks', 'novelty_artist_avg_year', 'mainstreaminess_avg_year']]
y = data['gender']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Standardize features (to improve model performance)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train a KMeans clustering model
num_clusters = 2  # Number of clusters (assuming 2 for gender)
model = KMeans(n_clusters=num_clusters, random_state=42)
model.fit(X_train_scaled)

# Predict the cluster labels for the test set
cluster_labels = model.predict(X_test_scaled)

# Convert cluster labels to strings ('f' and 'm')
cluster_labels_str = np.where(cluster_labels == 0, 'f', 'm')

# Calculate silhouette score to assess clustering quality
silhouette_avg = silhouette_score(X_test_scaled, cluster_labels)
print("Silhouette Score:", silhouette_avg)

# Evaluate cluster performance using classification report
print(classification_report(y_test, cluster_labels_str))

"""5. Random forest classifier (all 5 features) - Highest accuracy"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Select features (listening events and count of artists) and target variable (gender)
X = data[['cnt_listeningevents', 'cnt_distinct_artists', 'cnt_distinct_tracks', 'novelty_artist_avg_year', 'mainstreaminess_avg_year']]
y = data['gender']


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Initialize and train a Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate model performance
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Get feature importance
feature_importance = rf_model.feature_importances_
feature_names = X.columns

# Print feature importance
print("Feature Importance:")
for name, importance in zip(feature_names, feature_importance):
    print(f"{name}: {importance:.4f}")

df = pd.read_csv('LFM-1b_users_additional.txt', delimiter='\t' )
df = df[['cnt_listeningevents', 'cnt_distinct_artists','novelty_artist_avg_year']]
df.isna().sum()

# Load dataset
import numpy as np
df = pd.read_csv('LFM-1b_users_additional.txt', delimiter='\t' )
df_output_data_new = pd.read_csv('output_data_new.txt', delimiter='\t' )

df_add = df.rename (columns={'user-id':'user_id'})
data  = pd.merge(df_add, df_output_data_new, on='user_id')
data = data.replace('?',np.NaN)
data = data.dropna()

"""prediction classification model based on listening events and the count of artists

Actual - Imbalanced dataset

##1. Logistic Regression (2 input features)
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

# Select features (listening events and count of artists) and target variable (gender)
X = data[['cnt_listeningevents', 'cnt_distinct_artists']]
y = data['gender']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Standardize features (to improve model performance)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train a logistic regression model
model = LogisticRegression(random_state=42)
model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)

# Calculate accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Evaluate model performance
print(classification_report(y_test, y_pred))

"""Explanation of the report for the binary classification problem. This report contains various metrics that assess the performance of the model on both classes ("f" and "m").

Interpretation
1. Precision is the ratio of correctly predicted positive observations
to the total predicted positives. In the "m" class, the precision is 0.71, which means that when the model predicts "m," it is correct 71% of the time. However, in the "f" class, the precision is 0.00, indicating that the model doesn't predict the "f" class correctly at all.

2. Recall (Sensitivity): Recall is the ratio of correctly predicted positive observations to the all observations in the actual class. In the "m" class, the recall is 1.00, indicating that the model correctly captures all instances of the "m" class. However, in the "f" class, the recall is 0.00, suggesting that the model doesn't capture any instances of the "f" class.•••

3. F1-Score: The F1-score is the weighted average of precision and recall. It takes into account both false positives and false negatives. In the "m" class, the F1-score is 0.83, which indicates a reasonable balance between precision and recall. In the "f" class, the F1-score is 0.00, reflecting the poor performance of the model in this class.

4. Support: Support is the number of actual occurrences of the class in the specified dataset. In the "f" class, there are 3451 instances, and in the "m" class, there are 8446 instances.

5. Accuracy: Accuracy is the ratio of correctly predicted observations to the total observations. The overall accuracy is 0.71, meaning that the model correctly predicts the class for about 71% of the instances.

6. Macro Avg: Macro average calculates the average performance metrics across classes without considering class imbalance. In this case, the macro average precision is 0.35, recall is 0.50, and F1-score is 0.42.

7. Weighted Avg: Weighted average also calculates the average performance metrics across classes, but it takes into account class imbalance. In this case, the weighted average precision is 0.50, recall is 0.71, and F1-score is 0.59.

8. It's clear from the evaluation metrics that the model performs well for the "m" class but poorly for the "f" class. This could indicate a class imbalance issue or some other challenges with the data or the model itself. Further analysis and potentially model adjustments are needed to improve performance for both classes.

##2. Logistic Regression ( 3 input features )
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Separate features and target variable
X = data[['cnt_listeningevents', 'cnt_distinct_artists', 'cnt_distinct_tracks']]
y = data['gender']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize the Logistic Regression model
model = LogisticRegression()

# Train the model
model.fit(X_train_scaled, y_train)

# Get feature importances (coefficients) from the trained model
feature_importances = model.coef_[0]

# Create a DataFrame to display feature importances
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

print("Feature Importances:")
print(importance_df)

# Predict on the test set
y_pred = model.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:\n", report)

"""##3. Logistic regression (feature engineering - 4 features in all )"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

data['listening_per_artist'] = data['cnt_listeningevents'] / data['cnt_distinct_artists']

# Separate features and target variable
X = data[['cnt_listeningevents', 'cnt_distinct_artists', 'cnt_distinct_tracks','listening_per_artist' ]]
y = data['gender']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize the Logistic Regression model
model = LogisticRegression()

# Train the model
model.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred = model.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:\n", report)

"""#4. Logistic regression ( 4 features )"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report


# Separate features and target variable
X = data[['cnt_listeningevents', 'cnt_distinct_artists', 'cnt_distinct_tracks','cnt_listeningevents_per_week' ]]
y = data['gender']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize the Logistic Regression model
model = LogisticRegression()

# Train the model
model.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred = model.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:\n", report)

"""##5. Random Forest Classifier"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report


# Select features (listening events and count of artists) and target variable (gender)
X = data[['cnt_listeningevents', 'cnt_distinct_artists']]
y = data['gender']


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train a Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate model performance
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""1. Precision: For the "f" class, the precision is 0.34, indicating that out of the instances predicted as "f," only 34% are actually "f." For the "m" class, the precision is 0.72, meaning that 72% of the instances predicted as "m" are actually "m."
2. Recall (Sensitivity): The "f" class has a recall of 0.19, implying that only 19% of the actual "f" instances were correctly identified by the model. The "m" class has a recall of 0.85, indicating that the model correctly captures 85% of the actual "m" instances.
3. F1-Score: The F1-score for the "f" class is 0.24, which is the harmonic mean of precision and recall. It balances both false positives and false negatives. The F1-score for the "m" class is 0.78, which indicates a better balance between precision and recall.
4. Support: The "f" class has 2749 instances, and the "m" class has 6769 instances.
5. Accuracy: The overall accuracy of the model is 0.66, meaning that it correctly predicts the class for approximately 66% of the instances.
6. Macro Avg: The macro average calculates the average performance metrics across classes without considering class imbalance. In this case, the macro average precision is 0.53, recall is 0.52, and F1-score is 0.51.
7. Weighted Avg: The weighted average calculates the average performance metrics across classes, taking into account class imbalance. In this case, the weighted average precision is 0.61, recall is 0.66, and F1-score is 0.62.

The evaluation metrics suggest that the model performs reasonably well for the "m" class, with a higher precision and recall, but not as well for the "f" class. The lower precision and recall for the "f" class might indicate that the model struggles to correctly identify instances of this class. One of the  factors for this could be class imbalance.

When the classes are imbalanced, the weighted average metrics can be higher than the macro average metrics because the weighted average gives more emphasis to the larger class. If the larger class is performing well, it can pull up the weighted average metrics, even if some smaller classes are not performing as well.

In my research, the weighted average metrics are higher than the macro average metrics because the "m" class (the larger class) has better precision, recall, and F1-score compared to the "f" class. The better performance of the "m" class contributes more to the weighted average, leading to the observed difference. This is a common situation in cases where there's a significant class imbalance, and it's important to interpret the metrics in the context of the data distribution and the probllem at hand

- Class Imbalance: In many real-world scenarios, datasets may have an unequal distribution of classes. One class could have significantly more instances than the other. This class imbalance can impact the average metrics.
- Macro Average: The macro average calculates the average of performance metrics (precision, recall, F1-score, etc.) for each class separately and then averages those values. It treats each class equally, regardless of its actual frequency in the dataset. As a result, if one class is very small in terms of instances, it has the same impact on the macro average as a larger class.
- Weighted Average: The weighted average, on the other hand, calculates the average of performance metrics by considering the frequency (or "weight") of each class. It gives more weight to classes with a larger number of instances. This means that metrics for classes with more instances will have a larger influence on the weighted average.

##6. Random forest classifier and feature engineering
"""

data['listening_per_artist'] = data['cnt_listeningevents'] / data['cnt_distinct_artists']


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Select features (listening events and count of artists) and target variable (gender)
X = data[['cnt_listeningevents', 'cnt_distinct_artists','listening_per_artist']]
y = data['gender']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train a Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate model performance
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""##7. Random forest classifier (4 features)

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Select features (listening events and count of artists) and target variable (gender)
X = data[['cnt_listeningevents', 'cnt_distinct_artists','cnt_distinct_tracks','cnt_listeningevents_per_week' ]]
y = data['gender']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train a Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate model performance
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""8. Random forest classifier (all 5 features)"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Select features (listening events and count of artists) and target variable (gender)
X = data[['cnt_listeningevents', 'cnt_distinct_artists','cnt_distinct_tracks','cnt_listeningevents_per_week','mainstreaminess_avg_year']]
y = data['gender']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train a Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate model performance
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))